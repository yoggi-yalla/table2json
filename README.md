# jsonbuilder
This is a module used for converting Excel-like data to a structured JSON format. It makes heavy use of Pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) objects for data processing, and [asteval](https://newville.github.io/asteval/) for evaluating user input.

The intended audience is someone who receives a wide range of Excel-like input formats and wants to normalize all of them using a common framework.

## Example usage
```Python
import jsonbuilder

# Examples of all inputs can be found further down

csv = 'path/to/csv/file.csv' # Required
fmt = {
  "mapping": {TODO} # Required
  "functions": [TODO] # Optional
  "df_transforms": [TODO] # Optional
}

jbTree = jsonbuilder.Tree(fmt, csv)

output_json = jbTree.build().toJson(indent=2)
```
<br>

## Intro
Consider a simple .csv file with EURSEK fixing data:

|currency1|currency2|fixing |date      |
|---------|---------|-------|----------|
|EUR      |SEK      |10.8623|2020-04-20|
|EUR      |SEK      |10.9543|2020-04-21|
|EUR      |SEK      |10.9423|2020-04-22|
|EUR      |SEK      |10.8883|2020-04-23|
|EUR      |SEK      |10.8723|2020-04-24|

There are many ways of expressing this data in JSON format:

````python
# One example:
[
  {
    "currency1": "EUR",
    "currency2": "SEK",
    "fixing": 10.8623,
    "fixing_date": "2020-04-20"
  },
  {
    "currency1": "EUR",
    "currency2": "SEK",
    "fixing": 10.9543,
    "fixing_date": "2020-04-21"
  },
  ...
]

# Another example:
{
  "EURSEK": {
    "dates": [
      "2020-04-20",
      "2020-04-21",
      ...
    ],
    "fixings": [
      10.8623,
      10.9543,
      ...
    ]
  }
}
````

There are countless ways of converting the .csv into JSON format, some more sensible than others of course. If you have an application that consumes JSON data in a certain format, and you receive data in a flat .csv format, then this tool allows you to easily convert the .csv data to the specific JSON format that can be consumed by your application.

<br>

## Mapping
The mapping is in itself a JSON object, specifying the shape of the desired output JSON. The nomenclature used in this project is similar to the official JSON documentation, so if you are not familiar with it already I suggest you have a look at their [webpage](https://www.json.org/json-en.html). The mapping is best described as a tree of nodes, where each node can have the following attributes:

|Attribute|Description|
|-------------:|-----------|
|``type``          | Can be ``"object"``, ``"array"``, or ``"primitive"``, defaults to ``"primitive"``|
|``name``      | This is used to specify the name of a value within an object|
|``value``     | This is typically left blank but can be used for setting a hard-coded value on `primitive` nodes. May contain *any* valid JSON value such as ``"foo"`` or ``0.5`` or ``[true, false, 123]`` or ``{"foo":"bar"}`` etc.|
|``column``         | The column in the DataFrame containing the value to be extracted, e.g. ``"some_column_name"``. This can only be used on primitive nodes. |
|``children``      | An array of all child nodes. Any child of an ``object`` should have a name. Conversely, the children of an ``array`` should not have a name, and any provided name will be ignored. ``primitive`` nodes have no children.|
|``filter``        | Applies a filter to the DataFrame by checking for truth values, for example: <br>``"currency1 == 'EUR' and currency2 == 'SEK'"``. <br>See [df.query](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html) for more informaiton.|
|``group_by``      |  Should contain a column name, e.g. ``"some_other_column_name"``. The jsonbuilder Node will iterate over each unique group in this column and generate one value for each group.|
|``iterate``      |  Similar to ``group_by`` but it is much faster. This is a bool, and if set to ``true`` the jsonbuilder Node will build one value for each *row*. While doing so the jsonbuilder Node drops the DataFrame from memory, so it's not possible to use ``filter``, ``group_by``, or ``iterate`` on any descendant node.|
|``transmute``          | Allows the user to provide an arbitrary expression with ``x``, ``r``, and ``df`` as the variables at their disposal. The evaluated expression is assigned directly to the output value, for example: <br><br>``"x if r['date']>"2020-04-03" else 0"``<br><br>You can read more about the behavior [here](#Transmutes). It is normally a good idea to avoid complex transmutes and instead prepare the data as needed in the [transforms](#Transforms).|

<br>

### Here's an example mapping:
```python
mapping = \
{
  "type": "object",
  "children":[
    {
      "type": "array",
      "name": "fixings",
      "children": [
        {
          "type": "object",
          "iterate": true,
          "children": [
            {
              "name": "fixing",
              "column": "fixing"
            },
            {
              "name": "currency_pair",
              "transmute": "r['currency1']+r['currency2']"
            },
            {
              "name": "fixing_date",
              "column": "date"
            }
          ]
        }
      ]
    }
  ]
}
```
### Which would generate the following output:
```python
{
  "fixings": [
    {
      "currency_pair": "EURSEK",
      "fixing": 10.8623,
      "fixing_date": "2020-04-20"
    },
    {
      "currency_pair": "EURSEK",
      "fixing": 10.9543,
      "fixing_date": "2020-04-21"
    },
    ...
  ]
}
```

<br>

## Functions
The jsonbuilder module allows the user to define their own functions that can be accessed in the [transforms](#Transforms) and in the [transmutes](#Transmutes). They are passed as a list of named functions in a string format:
``` python
functions = [
  "def f1(x,r,df): y = len(df.index); x += y; return x",
  "def f2(df): return df.fillna(0)"
]
```
  

The functions can later be accessed like this: 
```python
mapping: {
  ...
  "transmute": "f1(x,r,df)"
  ...
}

transforms:[
  ...
  "f2(df)",
  ...
]
```

For simple operations it is often more convenient to write an expression directly in the ``"transmute"``-string, but for large/complex operations this functionality is essential.

<br>

## Transforms
Transforms are used for column-wise (or table-wise) transforms of the DataFrame before building the final JSON. They are passed as a list of expressions in a string format:
```python
transforms = [
  # Converts the fixings to Swedish Ã–re
  "df['fixing']*100",

  # Creates a new column called 'currency_pair'
  "(df['currency1'] + df['currency2']).rename('currency_pair')",

  # Replaces all NaN-values in the DataFrame with 0
  "df.fillna(0)",

  # User defined functions can be used here as well
  "f2(df)"
]
```

Here, `df` is used to access the DataFrame to be transformed. The expression is expected to evaluate to either a Pandas Series object (i.e. a column, see first two examples above) *or* to a DataFrame (see third example above). If it evaluates to a DataFrame it will *replace* the input DataFrame, if it evaluates to a Series the Series is *attached* to the DataFrame. The expressions will be executed one at a time in the order they appear in the list. 

If only one column is used in the transform then the output Series will have the same name as the input column, which means the corresponding column in the DataFrame will be _overwritten_ with the output Series.

If more than one column is used, the resulting Series will have an empty name. This Series should be renamed by the user to get a sensible column header, as shown in the second transform above. 

Renaming can also be useful if one wants to transform a single column, but save the output into a _new_ column.


## Transmutes

Transmutes allow the user to manipulate the value generated by one jsonbuilder Node before it gets returned to their parent. To fully understand this concept it helps to understand how the "build"-process works. When a node is asked to build itself, given a DataFrame, it will:

1. Apply the filter specified on this node to the DataFrame
2. Build itself, which for primitive nodes means either extracting one value from the DataFrame or returning a hard-coded value, and for objects or arrays mean:
    1. Initialize its value as an empty container (`{}` for objects, `[]` for arrays)
    2. Ask all of its children to build themselves based on the filtered DataFrame
    3. Add the value built by each child to the container
3. Transmute the value that was built
4. Return itself to the caller


This is slightly simplified, step 2 also involves iterating over rows or groups of the DataFrame. What that means in practice is that in each iteration, only one row or group of the dataframe will be passed down to the child node.

The transmute is an expression with three special variables: `x`, `r`, and `df`. Within the expression, `x` represents the value stored on this node, `r` represents the row currently being processed, and `df` represents the group (i.e. DataFrame) currently being processed. When iterating over groups, `r` represents the top row of the current group, but when iterating over rows there is no DataFrame available, so `df` is equal to `None`. 

Here are a few examples of how it may be used:

```Python
   # Multiply the value by 100
1. "transmute": "x*100"

   # Set value to r['fixing'] or fall back to x
2. "transmute": "r['fixing'] or x"

   # Set value to a list containing the entire fixings column
   # This is significantly faster than iterating over each row
3. "transmute": "df['fixings'].tolist()" 

   # x, r, and df can be passed to your own functions like this
4. "transmute": "f1(x,r,df)"
```

Keep in mind that `x` will be of different type depending on whether the transmute is specified on a `primitive-`, `object-`, or `array` node. Whatever the transmute evaluates to will be the final value of the node. 
